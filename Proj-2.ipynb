{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18CS30032_Assn2_NLP_A21.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "# **Assignment-2 for CS60075: Natural Language Processing**\n",
        "\n",
        "#### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
        "\n",
        "#### Date of Announcement: 15th Sept, 2021\n",
        "#### Deadline for Submission: 11.59pm on Wednesday, 22nd Sept, 2021 \n",
        "#### Submit this .ipynb file, named as `<Your_Roll_Number>_Assn2_NLP_A21.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.  This dataset consists of 50k movie reviews (25k positive, 25k negative). You can download the dataset from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONM5Q4SCe9Mr"
      },
      "source": [
        "Please submit with outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElRkQElWUMjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfdd830-6085-4c4c-d306-876f408b563d"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhHRim2AUm4z"
      },
      "source": [
        "#Load the IMDB dataset. You can load it using pandas as dataframe\n",
        "df = pd.read_csv('IMDB Dataset.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK_Hn2f6VMP7"
      },
      "source": [
        "# Preprocessing\n",
        "PrePrecessing that needs to be done on lower cased corpus\n",
        "\n",
        "1. Remove html tags\n",
        "2. Remove URLS\n",
        "3. Remove non alphanumeric character\n",
        "4. Remove Stopwords\n",
        "5. Perform stemming and lemmatization\n",
        "\n",
        "You can use regex from re. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9gxFb6uKXc7"
      },
      "source": [
        "def preprocess(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove html tags\n",
        "    text = re.sub('<[^<]+?>', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub('http\\S+|www.\\S+', '', text)\n",
        "    return text\n",
        "\n",
        "df['review'] = df['review'].apply(lambda text : preprocess(text))\n",
        "\n",
        "# Count the number of sentences, because afterwards '.' will be removed\n",
        "num_sentences = 0\n",
        "for idx, row in df.iterrows():\n",
        "    num_sentences += len(sent_tokenize(row['review']))\n",
        "\n",
        "# Remove non-alphanumeric characters\n",
        "df['review'] = df['review'].str.replace('\\W', ' ')\n",
        "df['review'] = df['review'].str.replace('\\s+', ' ')\n",
        "\n",
        "# Remove Stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "df['review'] = df['review'].apply(lambda text : ' '.join([word for word in text.split() if word not in (stopwords_list)]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbKsqUPeZ8X6"
      },
      "source": [
        "# Stemming\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    stem_sentence = []\n",
        "    for word in tokens:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "# Lemmatization\n",
        "Lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatizeSentence(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    lemmatize_sentence = []\n",
        "    for word in tokens:\n",
        "        lemmatize_sentence.append(Lemmatizer.lemmatize(word))\n",
        "        lemmatize_sentence.append(\" \")\n",
        "    return \"\".join(lemmatize_sentence)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu5dmVXE2-UE"
      },
      "source": [
        "for index, row in df.iterrows():\n",
        "    row['review'] = stemSentence(row['review'])\n",
        "    row['review'] = lemmatizeSentence(row['review'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaSkfcvYGXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefd52bd-21d6-4029-ece0-381ddd96c972"
      },
      "source": [
        "# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n",
        "num_words = 0\n",
        "num_positive = 0\n",
        "num_negative = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    num_words += len(row['review'].split())\n",
        "    if row['sentiment'] == 'positive':\n",
        "        num_positive += 1\n",
        "    else:\n",
        "        num_negative += 1\n",
        "\n",
        "print(\"Number of words in the reviews column =\", num_words)\n",
        "print(\"Number of sentences in the reviews column =\", num_sentences)\n",
        "print(\"Average Length of a sentence = {0:.4f}\".format(num_words / num_sentences))\n",
        "\n",
        "print(\"Number of positive sentiment examples =\", num_positive)\n",
        "print(\"Number of negative sentiment examples =\", num_negative)\n",
        "print(\"Proportion of data for positive sentiment = {0:.2f} %\".format((num_positive * 100) / (num_positive + num_negative)))\n",
        "print(\"Proportion of data for negative sentiment = {0:.2f} %\".format((num_negative * 100) / (num_positive + num_negative)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the reviews column = 5980737\n",
            "Number of sentences in the reviews column = 532492\n",
            "Average Length of a sentence = 11.2316\n",
            "Number of positive sentiment examples = 25000\n",
            "Number of negative sentiment examples = 25000\n",
            "Proportion of data for positive sentiment = 50.00 %\n",
            "Proportion of data for negative sentiment = 50.00 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FkJ-e2pUwun"
      },
      "source": [
        "# Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVq-mN28U_J4"
      },
      "source": [
        "# get reviews column from df\n",
        "reviews = df['review']\n",
        "\n",
        "# get labels column from df\n",
        "labels = df['sentiment']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljo5NquhXTXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0845b45a-5daf-4ef2-d74e-da99f5fdddd8"
      },
      "source": [
        "# Use label encoder to encode labels. Convert to 0/1\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "# print(enc.classes_)\n",
        "print(encoder.classes_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['negative' 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzG-C_EVWWET"
      },
      "source": [
        "# Split the data into train and test (80% - 20%). \n",
        "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
        "\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels,\n",
        "                                                                              stratify = encoded_labels, test_size = 0.2,\n",
        "                                                                              shuffle = True, random_state = 42)\n",
        "\n",
        "train_sentences = train_sentences.to_numpy()\n",
        "test_sentences = test_sentences.to_numpy()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz1YdsSkiWCX"
      },
      "source": [
        "Here there are two approaches possible for building vocabulary for the naive Bayes.\n",
        "1. Take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n",
        "2. Take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one the probability term is not zero.\n",
        " \n",
        "You are supposed to go by the 2nd approach.\n",
        " \n",
        "Also building vocab by taking all words in the train set is memory intensive, hence you are required to build vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
        "\n",
        "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
        "\n",
        "\n",
        "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
        "\n",
        "$N_{w_j}$ : Total count of features in class $w_j$\n",
        "\n",
        "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
        "\n",
        "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cllNfGmUr77"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Use Count vectorizer to get frequency of the words\n",
        "\n",
        "vec = CountVectorizer(max_features = 3000)\n",
        "doc_term_freq = vec.fit_transform(train_sentences).toarray()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzRvPjWaWUnm"
      },
      "source": [
        "# Use laplace smoothing for words in test set not present in vocab of train set\n",
        "vocab = vec.vocabulary_\n",
        "V = len(vocab)\n",
        "\n",
        "class_wise_term_freq = np.zeros((2, V), dtype = int)\n",
        "\n",
        "for i in range(len(doc_term_freq)):\n",
        "    classification_of_doc = train_labels[i]\n",
        "    for j in range(V):\n",
        "        class_wise_term_freq[classification_of_doc][j] += doc_term_freq[i][j]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yovqqsv9P1J"
      },
      "source": [
        "# Build the model. Don't use the model from sklearn\n",
        "import math\n",
        "\n",
        "total_count_of_features = np.zeros((2, 1), dtype = int)\n",
        "for i in range(2):\n",
        "    for j in range(V):\n",
        "        total_count_of_features[i] += class_wise_term_freq[i][j]\n",
        "\n",
        "freq_of_class = np.zeros((2, 1), dtype = int)\n",
        "for i in range(len(doc_term_freq)):\n",
        "    freq_of_class[train_labels[i]] += 1\n",
        "\n",
        "prob_of_class = np.zeros((2, 1))\n",
        "for i in range(2):\n",
        "    prob_of_class[i] = freq_of_class[i] / len(doc_term_freq)\n",
        "\n",
        "def predict(sentence):\n",
        "    list_of_tokens = word_tokenize(sentence)\n",
        "    prob_class = np.ones((2, 1))\n",
        "    for i in range(2):\n",
        "        prob_class[i] = math.log10(prob_of_class[i])\n",
        "    \n",
        "    D0 = total_count_of_features[0] + V\n",
        "    D1 = total_count_of_features[1] + V\n",
        "    \n",
        "    for token in list_of_tokens:\n",
        "        if token in vocab.keys():\n",
        "            term_idx = vocab[token]\n",
        "            prob_class[0] += math.log10(1 + class_wise_term_freq[0][term_idx]) - math.log10(D0)\n",
        "            prob_class[1] += math.log10(1 + class_wise_term_freq[1][term_idx]) - math.log10(D1)\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    if prob_class[1] > prob_class[0]:\n",
        "        return 1\n",
        "    return 0\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtQSl1zvW4DD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4449f0-41df-425a-eeb0-3ca97de666ec"
      },
      "source": [
        "# Test the model on test set and report Accuracy\n",
        "\n",
        "correct_classsifications = 0\n",
        "num_test_sentences = len(test_sentences)\n",
        "\n",
        "for i in range(num_test_sentences):\n",
        "    predicted_classification = predict(test_sentences[i])\n",
        "    if predicted_classification == test_labels[i]:\n",
        "        correct_classsifications += 1\n",
        "\n",
        "print(\"Accuracy for Naive Bayes Classifier = {:.4f} %\".format(correct_classsifications * 100 / num_test_sentences))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Naive Bayes Classifier = 84.4000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNql0acU7sa"
      },
      "source": [
        "# *LSTM* based Classifier\n",
        "\n",
        "Use the above train and test splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkqnvbUOXoN0"
      },
      "source": [
        "# Hyperparameters of the model\n",
        "vocab_size = V\n",
        "oov_tok = '<OOK>'\n",
        "embedding_dim = 100\n",
        "max_length = 150\n",
        "padding_type='post'\n",
        "trunc_type='post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeycEg9nZAOF"
      },
      "source": [
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# convert train dataset to sequence and pad sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# convert Test dataset to sequence and pad sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtw3w895ZP39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb53868c-b367-430a-f9e7-8d7c2aba7186"
      },
      "source": [
        "# model initialization\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 150, 100)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                3096      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 387,601\n",
            "Trainable params: 387,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skmaDJMnZTzc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262be22a-da38-4f0a-db77-1a03f65bc204"
      },
      "source": [
        "num_epochs = 5\n",
        "history = model.fit(train_padded, train_labels, \n",
        "                    epochs=num_epochs, verbose=1, \n",
        "                    validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1125/1125 [==============================] - 187s 163ms/step - loss: 0.3984 - accuracy: 0.8181 - val_loss: 0.3782 - val_accuracy: 0.8328\n",
            "Epoch 2/5\n",
            "1125/1125 [==============================] - 185s 164ms/step - loss: 0.2751 - accuracy: 0.8901 - val_loss: 0.3052 - val_accuracy: 0.8760\n",
            "Epoch 3/5\n",
            "1125/1125 [==============================] - 182s 162ms/step - loss: 0.2369 - accuracy: 0.9064 - val_loss: 0.3079 - val_accuracy: 0.8720\n",
            "Epoch 4/5\n",
            "1125/1125 [==============================] - 185s 164ms/step - loss: 0.2007 - accuracy: 0.9216 - val_loss: 0.3290 - val_accuracy: 0.8735\n",
            "Epoch 5/5\n",
            "1125/1125 [==============================] - 183s 163ms/step - loss: 0.1695 - accuracy: 0.9370 - val_loss: 0.3506 - val_accuracy: 0.8665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEhWEr5Zq7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2640971-14b4-4d83-a26c-4c560a7d1b5c"
      },
      "source": [
        "# Calculate accuracy on Test data\n",
        "# Get probabilities\n",
        "prediction = model.predict(test_padded)\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "lstm_predicted_labels = np.zeros((len(prediction), 1), dtype = int)\n",
        "\n",
        "for i in range(len(prediction)):\n",
        "    if prediction[i][0] >= 0.5:\n",
        "        lstm_predicted_labels[i][0] = 1\n",
        "    else:\n",
        "        lstm_predicted_labels[i][0] = 0\n",
        "\n",
        "# Accuracy : one can use classification_report from sklearn\n",
        "print(\"Accuracy = {:.4f} %\".format(100 * accuracy_score(test_labels, lstm_predicted_labels)))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, lstm_predicted_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 86.5700 %\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.87      5000\n",
            "           1       0.87      0.86      0.86      5000\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIICV-ySOYL0"
      },
      "source": [
        "## Get predictions for random examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2RmfNL3OYL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e7a852-8e92-4f45-c02e-b987e4d5351b"
      },
      "source": [
        "# reviews on which we need to predict\n",
        "sentence = [\"The movie was very touching and heart whelming\", \n",
        "            \"I have never seen a terrible movie like this\", \n",
        "            \"the movie plot is terrible but it had good acting\"]\n",
        "\n",
        "def preprocess_random_sentences(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove html tags\n",
        "    text = re.sub('<[^<]+?>', '', text)\n",
        "    # Remove apostrophes\n",
        "    text = re.sub('\\'.*?\\s', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub('http\\S+|www.\\S+', '', text)\n",
        "    # Remove Stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in (stopwords_list)])\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = stemSentence(text)\n",
        "    text = lemmatizeSentence(text)\n",
        "    return text\n",
        "\n",
        "for i in range(3):\n",
        "    sentence[i] = preprocess_random_sentences(sentence[i])\n",
        "\n",
        "# convert to a sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "# pad the sequence\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# Get probabilities\n",
        "prediction_unseen = model.predict(padded)\n",
        "print(\"The probabilities are the following:\")\n",
        "print(prediction_unseen)\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "lstm_predicted_labels_unseen = np.zeros((len(prediction_unseen), 1), dtype = int)\n",
        "\n",
        "for i in range(len(prediction_unseen)):\n",
        "    if prediction_unseen[i][0] >= 0.5:\n",
        "        lstm_predicted_labels_unseen[i][0] = 1\n",
        "    else:\n",
        "        lstm_predicted_labels_unseen[i][0] = 0\n",
        "\n",
        "print(\"The labels are the following:\")\n",
        "print(lstm_predicted_labels_unseen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probabilities are the following:\n",
            "[[0.95695686]\n",
            " [0.18327245]\n",
            " [0.07588911]]\n",
            "The labels are the following:\n",
            "[[1]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_o7qv0iA95k"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "The accuracy of Naive Bayes classifier is 84.4 % whereas the accuracy of LSTM classifier is 86.6 %. LSTM is a bit better in terms of accuracy but depending upon the utility, Naive Bayes may also be useful because of it's simplicity and competitive accuracy."
      ]
    }
  ]
}