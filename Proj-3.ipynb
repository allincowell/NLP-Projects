{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment 3 on Natural Language Processing\n\n## Date : 23rd October, 2021\n\n### Instructor : Prof. Sudeshna Sarkar\n\n### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Desai","metadata":{"id":"TUo_mkiioAvN"}},{"cell_type":"markdown","source":"The central idea of this assignment is to experiment  Name Entity Recogniztion using pretrained language model BERT. Please split the NER data with train and validation, and evaluate the model on the validation.","metadata":{"id":"JwxSPrEspDmn"}},{"cell_type":"markdown","source":"Please submit with outputs. Submissions without predicted outputs will be penalized. **Please install necessary packages according to the task.**","metadata":{"id":"zoDPJlDJpmB8"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\n#print(os.listdir(\"../input\"))\n!pip install pytorch-pretrained-bert\n!pip install seqeval\n\n# Any results you write to the current directory are saved as output.","metadata":{"id":"ilwxcEJToM48","outputId":"fd4bc04d-db10-4ce1-f782-911d348de45a","execution":{"iopub.status.busy":"2021-11-06T16:42:12.901263Z","iopub.execute_input":"2021-11-06T16:42:12.901825Z","iopub.status.idle":"2021-11-06T16:42:40.931438Z","shell.execute_reply.started":"2021-11-06T16:42:12.901718Z","shell.execute_reply":"2021-11-06T16:42:40.930312Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pytorch-pretrained-bert\n  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n\u001b[K     |████████████████████████████████| 123 kB 630 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (1.9.1)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (1.19.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (1.19.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (2.25.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (2021.8.28)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pytorch-pretrained-bert) (4.62.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.10.0.2)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\nRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch-pretrained-bert) (0.5.0)\nRequirement already satisfied: botocore<1.23.0,>=1.22.4 in /opt/conda/lib/python3.7/site-packages (from boto3->pytorch-pretrained-bert) (1.22.4)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.23.0,>=1.22.4->boto3->pytorch-pretrained-bert) (2.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.23.0,>=1.22.4->boto3->pytorch-pretrained-bert) (1.26.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.4->boto3->pytorch-pretrained-bert) (1.16.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert) (2.10)\nInstalling collected packages: pytorch-pretrained-bert\nSuccessfully installed pytorch-pretrained-bert-0.6.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[K     |████████████████████████████████| 43 kB 204 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval) (1.19.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval) (0.23.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.1)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=21db6755a9bda08aa8d973a8c6944c26dd5a6319a8932b84774f325a04c0b543\n  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Read the data","metadata":{"id":"bRriQnXBn-2z"}},{"cell_type":"code","source":"dframe = pd.read_csv(\"A3_dataset.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"cAgLy2qp9TRu","outputId":"78f37bf5-5425-40a3-d901-f0e7faed96c8","execution":{"iopub.status.busy":"2021-11-06T16:43:25.722195Z","iopub.execute_input":"2021-11-06T16:43:25.725406Z","iopub.status.idle":"2021-11-06T16:43:31.204479Z","shell.execute_reply.started":"2021-11-06T16:43:25.725351Z","shell.execute_reply":"2021-11-06T16:43:31.203386Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n\n\n  exec(code_obj, self.user_global_ns, self.user_ns)\nb'Skipping line 281837: expected 25 fields, saw 34\\n'\n","output_type":"stream"}]},{"cell_type":"code","source":"dframe.head()","metadata":{"_uuid":"cb9e29cd3d9de99b21ebfedb930c4d59e813664d","id":"lxWnzF3e9TRw","outputId":"2c0b6445-73ec-4cad-e7d9-c2e40a4d6e2b","execution":{"iopub.status.busy":"2021-11-06T16:43:33.249309Z","iopub.execute_input":"2021-11-06T16:43:33.249614Z","iopub.status.idle":"2021-11-06T16:43:33.300550Z","shell.execute_reply.started":"2021-11-06T16:43:33.249563Z","shell.execute_reply":"2021-11-06T16:43:33.298798Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0     lemma next-lemma next-next-lemma next-next-pos  \\\n0           0  thousand         of        demonstr           NNS   \n1           1        of   demonstr            have           VBP   \n2           2  demonstr       have           march           VBN   \n3           3      have      march         through            IN   \n4           4     march    through          london           NNP   \n\n  next-next-shape next-next-word next-pos next-shape      next-word  ...  \\\n0       lowercase  demonstrators       IN  lowercase             of  ...   \n1       lowercase           have      NNS  lowercase  demonstrators  ...   \n2       lowercase        marched      VBP  lowercase           have  ...   \n3       lowercase        through      VBN  lowercase        marched  ...   \n4     capitalized         London       IN  lowercase        through  ...   \n\n  prev-prev-lemma prev-prev-pos prev-prev-shape prev-prev-word   prev-shape  \\\n0      __start2__    __START2__        wildcard     __START2__     wildcard   \n1      __start1__    __START1__        wildcard     __START1__  capitalized   \n2        thousand           NNS     capitalized      Thousands    lowercase   \n3              of            IN       lowercase             of    lowercase   \n4        demonstr           NNS       lowercase  demonstrators    lowercase   \n\n       prev-word sentence_idx        shape           word tag  \n0     __START1__          1.0  capitalized      Thousands   O  \n1      Thousands          1.0    lowercase             of   O  \n2             of          1.0    lowercase  demonstrators   O  \n3  demonstrators          1.0    lowercase           have   O  \n4           have          1.0    lowercase        marched   O  \n\n[5 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>lemma</th>\n      <th>next-lemma</th>\n      <th>next-next-lemma</th>\n      <th>next-next-pos</th>\n      <th>next-next-shape</th>\n      <th>next-next-word</th>\n      <th>next-pos</th>\n      <th>next-shape</th>\n      <th>next-word</th>\n      <th>...</th>\n      <th>prev-prev-lemma</th>\n      <th>prev-prev-pos</th>\n      <th>prev-prev-shape</th>\n      <th>prev-prev-word</th>\n      <th>prev-shape</th>\n      <th>prev-word</th>\n      <th>sentence_idx</th>\n      <th>shape</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>thousand</td>\n      <td>of</td>\n      <td>demonstr</td>\n      <td>NNS</td>\n      <td>lowercase</td>\n      <td>demonstrators</td>\n      <td>IN</td>\n      <td>lowercase</td>\n      <td>of</td>\n      <td>...</td>\n      <td>__start2__</td>\n      <td>__START2__</td>\n      <td>wildcard</td>\n      <td>__START2__</td>\n      <td>wildcard</td>\n      <td>__START1__</td>\n      <td>1.0</td>\n      <td>capitalized</td>\n      <td>Thousands</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>of</td>\n      <td>demonstr</td>\n      <td>have</td>\n      <td>VBP</td>\n      <td>lowercase</td>\n      <td>have</td>\n      <td>NNS</td>\n      <td>lowercase</td>\n      <td>demonstrators</td>\n      <td>...</td>\n      <td>__start1__</td>\n      <td>__START1__</td>\n      <td>wildcard</td>\n      <td>__START1__</td>\n      <td>capitalized</td>\n      <td>Thousands</td>\n      <td>1.0</td>\n      <td>lowercase</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>demonstr</td>\n      <td>have</td>\n      <td>march</td>\n      <td>VBN</td>\n      <td>lowercase</td>\n      <td>marched</td>\n      <td>VBP</td>\n      <td>lowercase</td>\n      <td>have</td>\n      <td>...</td>\n      <td>thousand</td>\n      <td>NNS</td>\n      <td>capitalized</td>\n      <td>Thousands</td>\n      <td>lowercase</td>\n      <td>of</td>\n      <td>1.0</td>\n      <td>lowercase</td>\n      <td>demonstrators</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>have</td>\n      <td>march</td>\n      <td>through</td>\n      <td>IN</td>\n      <td>lowercase</td>\n      <td>through</td>\n      <td>VBN</td>\n      <td>lowercase</td>\n      <td>marched</td>\n      <td>...</td>\n      <td>of</td>\n      <td>IN</td>\n      <td>lowercase</td>\n      <td>of</td>\n      <td>lowercase</td>\n      <td>demonstrators</td>\n      <td>1.0</td>\n      <td>lowercase</td>\n      <td>have</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>march</td>\n      <td>through</td>\n      <td>london</td>\n      <td>NNP</td>\n      <td>capitalized</td>\n      <td>London</td>\n      <td>IN</td>\n      <td>lowercase</td>\n      <td>through</td>\n      <td>...</td>\n      <td>demonstr</td>\n      <td>NNS</td>\n      <td>lowercase</td>\n      <td>demonstrators</td>\n      <td>lowercase</td>\n      <td>have</td>\n      <td>1.0</td>\n      <td>lowercase</td>\n      <td>marched</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset=dframe.drop(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n       'next-word', 'prev-iob', 'prev-lemma', 'prev-pos',\n       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n       'prev-prev-word', 'prev-shape', 'prev-word','shape'],axis=1)\n\n#dataset = dframe.dropdframe.drop([])","metadata":{"_uuid":"c598cf2c955b71ac3371bf6ebb19169b5734e5f5","id":"psG8otB69TRw","execution":{"iopub.status.busy":"2021-11-06T16:43:36.204722Z","iopub.execute_input":"2021-11-06T16:43:36.205000Z","iopub.status.idle":"2021-11-06T16:43:36.239279Z","shell.execute_reply.started":"2021-11-06T16:43:36.204970Z","shell.execute_reply":"2021-11-06T16:43:36.238296Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"_uuid":"c31cfa31598d78e73db241816ee8f7d98e7b320f","id":"5tFVIgyy9TRx","outputId":"8cdbc924-fc22-417b-ea0d-b1216d938ae2","execution":{"iopub.status.busy":"2021-11-06T16:43:39.245100Z","iopub.execute_input":"2021-11-06T16:43:39.245855Z","iopub.status.idle":"2021-11-06T16:43:39.260230Z","shell.execute_reply.started":"2021-11-06T16:43:39.245819Z","shell.execute_reply":"2021-11-06T16:43:39.258935Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   pos  sentence_idx           word tag\n0  NNS           1.0      Thousands   O\n1   IN           1.0             of   O\n2  NNS           1.0  demonstrators   O\n3  VBP           1.0           have   O\n4  VBN           1.0        marched   O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pos</th>\n      <th>sentence_idx</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NNS</td>\n      <td>1.0</td>\n      <td>Thousands</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>IN</td>\n      <td>1.0</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NNS</td>\n      <td>1.0</td>\n      <td>demonstrators</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>VBP</td>\n      <td>1.0</td>\n      <td>have</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>VBN</td>\n      <td>1.0</td>\n      <td>marched</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Preprocess the dataset","metadata":{"id":"B4gvBBQvqMra"}},{"cell_type":"code","source":"class SentenceGetter(object):\n    \n    def __init__(self, dataset):\n        self.n_sent = 1\n        self.dataset = dataset\n        self.empty = False\n        agg_func = lambda s: [(w,p, t) for w,p, t in zip(s[\"word\"].values.tolist(),\n                                                       s['pos'].values.tolist(),\n                                                        s[\"tag\"].values.tolist())]\n        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","metadata":{"_uuid":"27bac58d6d0a7c67a2e7ca71329ec89b86006cdc","id":"zq9n23HS9TRx","execution":{"iopub.status.busy":"2021-11-06T16:43:41.637403Z","iopub.execute_input":"2021-11-06T16:43:41.637723Z","iopub.status.idle":"2021-11-06T16:43:41.648601Z","shell.execute_reply.started":"2021-11-06T16:43:41.637692Z","shell.execute_reply":"2021-11-06T16:43:41.647284Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"getter = SentenceGetter(dataset)","metadata":{"_uuid":"ce9bb377d28f1062740ed52955e4473b9a91a915","id":"cjo3lxMD9TRy","execution":{"iopub.status.busy":"2021-11-06T16:43:45.723539Z","iopub.execute_input":"2021-11-06T16:43:45.723860Z","iopub.status.idle":"2021-11-06T16:43:52.788220Z","shell.execute_reply.started":"2021-11-06T16:43:45.723830Z","shell.execute_reply":"2021-11-06T16:43:52.787171Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\nsentences[0]","metadata":{"_uuid":"9aac920912c2846b8f01e7a9fac7a1dd75f470e5","id":"xVlKwn649TRz","outputId":"918c5fee-69ee-462c-ea72-93dd273456c1","execution":{"iopub.status.busy":"2021-11-06T16:43:54.789862Z","iopub.execute_input":"2021-11-06T16:43:54.790237Z","iopub.status.idle":"2021-11-06T16:43:54.931855Z","shell.execute_reply.started":"2021-11-06T16:43:54.790206Z","shell.execute_reply":"2021-11-06T16:43:54.930868Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country . Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'"},"metadata":{}}]},{"cell_type":"code","source":"labels = [[s[2] for s in sent] for sent in getter.sentences]\nprint(labels[0])","metadata":{"_uuid":"ef108c6958dccc1855966ece23d582a0a427ffce","id":"2rZRrXAA9TR0","outputId":"fa8572ca-ef28-4d6a-bedf-3d3659c75e2b","execution":{"iopub.status.busy":"2021-11-06T16:43:57.945732Z","iopub.execute_input":"2021-11-06T16:43:57.946871Z","iopub.status.idle":"2021-11-06T16:43:58.043213Z","shell.execute_reply.started":"2021-11-06T16:43:57.946810Z","shell.execute_reply":"2021-11-06T16:43:58.042191Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n","output_type":"stream"}]},{"cell_type":"code","source":"tags_vals = list(set(dataset[\"tag\"].values))\ntag2idx = {t: i for i, t in enumerate(tags_vals)}","metadata":{"_uuid":"7e274ec75490323bdbc5f0227d62fd17d3de90da","id":"RpGTxb6o9TR2","execution":{"iopub.status.busy":"2021-11-06T16:44:01.578728Z","iopub.execute_input":"2021-11-06T16:44:01.579530Z","iopub.status.idle":"2021-11-06T16:44:01.624223Z","shell.execute_reply.started":"2021-11-06T16:44:01.579497Z","shell.execute_reply":"2021-11-06T16:44:01.622927Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Import model pacakages","metadata":{"id":"1MOg7AYgqW7Z"}},{"cell_type":"code","source":"import torch\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertForTokenClassification, BertAdam\nfrom tqdm import tqdm, trange","metadata":{"_uuid":"1cdd149a0e14d9e9291bdf993f01c6fedb02d6ac","id":"CNIUugH29TR3","execution":{"iopub.status.busy":"2021-11-06T16:44:05.048200Z","iopub.execute_input":"2021-11-06T16:44:05.049234Z","iopub.status.idle":"2021-11-06T16:44:13.228525Z","shell.execute_reply.started":"2021-11-06T16:44:05.049173Z","shell.execute_reply":"2021-11-06T16:44:13.226446Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Set up seed value","metadata":{"id":"pMkMbt9dq7Qx"}},{"cell_type":"code","source":"# setup random seed\nimport random\n\nseed=42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"v9roxUsuq-s6","execution":{"iopub.status.busy":"2021-11-06T16:44:51.220883Z","iopub.execute_input":"2021-11-06T16:44:51.221458Z","iopub.status.idle":"2021-11-06T16:44:51.231664Z","shell.execute_reply.started":"2021-11-06T16:44:51.221423Z","shell.execute_reply":"2021-11-06T16:44:51.230056Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 40\nbs = 64","metadata":{"_uuid":"9bf2b8a2864802dae72ff7313e833a2c4d26fe73","id":"jUd3mFKc9TR4","execution":{"iopub.status.busy":"2021-11-06T16:44:53.044024Z","iopub.execute_input":"2021-11-06T16:44:53.044367Z","iopub.status.idle":"2021-11-06T16:44:53.049367Z","shell.execute_reply.started":"2021-11-06T16:44:53.044325Z","shell.execute_reply":"2021-11-06T16:44:53.048215Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()","metadata":{"_uuid":"f57a27970f56c488884c464ab87ac8228e4a0e72","id":"iS88Oaz99TR4","execution":{"iopub.status.busy":"2021-11-06T16:44:55.376437Z","iopub.execute_input":"2021-11-06T16:44:55.376841Z","iopub.status.idle":"2021-11-06T16:44:55.434742Z","shell.execute_reply.started":"2021-11-06T16:44:55.376811Z","shell.execute_reply":"2021-11-06T16:44:55.433644Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"torch.cuda.get_device_name(0)","metadata":{"_uuid":"01c72c953cf95d447e1376dfc5b7d349868bb0a6","id":"em4XP5la9TR5","outputId":"b267ab1a-c80d-4037-fb8a-b76b2b7e032a","execution":{"iopub.status.busy":"2021-11-06T16:44:58.430248Z","iopub.execute_input":"2021-11-06T16:44:58.430756Z","iopub.status.idle":"2021-11-06T16:44:58.440792Z","shell.execute_reply.started":"2021-11-06T16:44:58.430722Z","shell.execute_reply":"2021-11-06T16:44:58.439661Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'Tesla P100-PCIE-16GB'"},"metadata":{}}]},{"cell_type":"markdown","source":"Features generation  for pre-trained languge BERT","metadata":{"id":"RrW_k60XrUQ1"}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"_uuid":"115b9ebaeeb38126bd30bb825e00fdf2220c20df","id":"Q2lil3kW9TR5","outputId":"9a740ef5-e65a-4d39-dea2-1d982195d7dd","execution":{"iopub.status.busy":"2021-11-06T16:45:01.575991Z","iopub.execute_input":"2021-11-06T16:45:01.576294Z","iopub.status.idle":"2021-11-06T16:45:02.895086Z","shell.execute_reply.started":"2021-11-06T16:45:01.576262Z","shell.execute_reply":"2021-11-06T16:45:02.893853Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|██████████| 231508/231508 [00:00<00:00, 712447.71B/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint(tokenized_texts[0])","metadata":{"_uuid":"e1a671a466e656bdf14158365d0fc66273abf6ae","id":"4fPn_NPU9TR5","outputId":"e9b1c1e4-9016-4924-d654-d8ae1af7493d","execution":{"iopub.status.busy":"2021-11-06T16:45:06.705952Z","iopub.execute_input":"2021-11-06T16:45:06.706265Z","iopub.status.idle":"2021-11-06T16:45:36.402711Z","shell.execute_reply.started":"2021-11-06T16:45:06.706234Z","shell.execute_reply":"2021-11-06T16:45:36.401671Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.', 'thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")","metadata":{"_uuid":"f77f0f8f9c779c15b49790cf9c42becef3f39cba","id":"6cS5w4PS9TR5","execution":{"iopub.status.busy":"2021-11-06T16:45:44.084682Z","iopub.execute_input":"2021-11-06T16:45:44.085663Z","iopub.status.idle":"2021-11-06T16:45:45.090115Z","shell.execute_reply.started":"2021-11-06T16:45:44.085624Z","shell.execute_reply":"2021-11-06T16:45:45.088975Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n                     dtype=\"long\", truncating=\"post\")","metadata":{"_uuid":"7f28fe1c656c371e47b23a6d905527da9895d3ee","id":"C5oXcoOb9TR6","execution":{"iopub.status.busy":"2021-11-06T16:45:49.919790Z","iopub.execute_input":"2021-11-06T16:45:49.920074Z","iopub.status.idle":"2021-11-06T16:45:50.431146Z","shell.execute_reply.started":"2021-11-06T16:45:49.920045Z","shell.execute_reply":"2021-11-06T16:45:50.430205Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"attention_masks = [[float(i>0) for i in ii] for ii in input_ids]","metadata":{"_uuid":"42250d53714a7a30d23778ff0692ee7aed5018b3","id":"sc8MJKdv9TR7","execution":{"iopub.status.busy":"2021-11-06T16:45:53.916036Z","iopub.execute_input":"2021-11-06T16:45:53.916788Z","iopub.status.idle":"2021-11-06T16:45:56.110474Z","shell.execute_reply.started":"2021-11-06T16:45:53.916752Z","shell.execute_reply":"2021-11-06T16:45:56.109406Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n                                                            random_state=2018, test_size=0.1)\ntr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=2018, test_size=0.1)","metadata":{"_uuid":"7983cec40eaf72173ff1de156a2e5bc5f746f6b6","id":"mgLwrHgK9TR7","execution":{"iopub.status.busy":"2021-11-06T16:45:58.093190Z","iopub.execute_input":"2021-11-06T16:45:58.093645Z","iopub.status.idle":"2021-11-06T16:45:58.141134Z","shell.execute_reply.started":"2021-11-06T16:45:58.093579Z","shell.execute_reply":"2021-11-06T16:45:58.140235Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"tr_inputs = torch.tensor(tr_inputs)\nval_inputs = torch.tensor(val_inputs)\ntr_tags = torch.tensor(tr_tags)\nval_tags = torch.tensor(val_tags)\ntr_masks = torch.tensor(tr_masks)\nval_masks = torch.tensor(val_masks)","metadata":{"_uuid":"809efdd5002265fad91a3c2e5b019d14e48987c2","id":"JfPfWHpb9TR7","execution":{"iopub.status.busy":"2021-11-06T16:46:00.118085Z","iopub.execute_input":"2021-11-06T16:46:00.118725Z","iopub.status.idle":"2021-11-06T16:46:00.308679Z","shell.execute_reply.started":"2021-11-06T16:46:00.118691Z","shell.execute_reply":"2021-11-06T16:46:00.307640Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n\nvalid_data = TensorDataset(val_inputs, val_masks, val_tags)\nvalid_sampler = SequentialSampler(valid_data)\nvalid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)","metadata":{"_uuid":"a7d0ab1dd92524ede3f8cfd868d47fa854e3b7f7","id":"qb46kFc09TR7","execution":{"iopub.status.busy":"2021-11-06T16:46:02.240214Z","iopub.execute_input":"2021-11-06T16:46:02.241306Z","iopub.status.idle":"2021-11-06T16:46:02.249510Z","shell.execute_reply.started":"2021-11-06T16:46:02.241264Z","shell.execute_reply":"2021-11-06T16:46:02.248295Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))","metadata":{"_uuid":"907a1068525b1c1fd64cd5641af29fdcff498c89","id":"zc2fkUYJ9TR7","outputId":"a399ed29-0f02-47f0-a407-f5ddee95a4dd","execution":{"iopub.status.busy":"2021-11-06T16:46:05.299160Z","iopub.execute_input":"2021-11-06T16:46:05.300227Z","iopub.status.idle":"2021-11-06T16:46:30.455522Z","shell.execute_reply.started":"2021-11-06T16:46:05.300178Z","shell.execute_reply":"2021-11-06T16:46:30.454510Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"100%|██████████| 407873900/407873900 [00:15<00:00, 27121509.72B/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"model.cuda()","metadata":{"_uuid":"553fc0c1015878731b5c2ab4185026bd44101695","id":"MgvCcJ1k9TR8","outputId":"d486fa31-c4b9-4d27-9cdc-dedaf773550b","execution":{"iopub.status.busy":"2021-11-06T16:46:32.447359Z","iopub.execute_input":"2021-11-06T16:46:32.448287Z","iopub.status.idle":"2021-11-06T16:46:37.992039Z","shell.execute_reply.started":"2021-11-06T16:46:32.448235Z","shell.execute_reply":"2021-11-06T16:46:37.991056Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=18, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"param_optimizer = list(model.classifier.named_parameters()) \noptimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\noptimizer = Adam(optimizer_grouped_parameters, lr=3e-5)","metadata":{"_uuid":"c32ef5fb14eee369b8b945f48316b8f973a812d6","id":"Ag1OLUtg9TR8","execution":{"iopub.status.busy":"2021-11-06T16:46:50.126095Z","iopub.execute_input":"2021-11-06T16:46:50.126409Z","iopub.status.idle":"2021-11-06T16:46:50.133434Z","shell.execute_reply.started":"2021-11-06T16:46:50.126364Z","shell.execute_reply":"2021-11-06T16:46:50.132180Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import f1_score\n\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=2).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"_uuid":"3486fe719942f089d4041d6db192154cd4ba6052","id":"4fSU0CFV9TR9","execution":{"iopub.status.busy":"2021-11-06T16:46:53.171252Z","iopub.execute_input":"2021-11-06T16:46:53.172113Z","iopub.status.idle":"2021-11-06T16:46:53.184189Z","shell.execute_reply.started":"2021-11-06T16:46:53.172075Z","shell.execute_reply":"2021-11-06T16:46:53.183021Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"5VJHsPlWubDN"}},{"cell_type":"code","source":"epochs = 5\nmax_grad_norm = 1.0\n\nfor _ in trange(epochs, desc=\"Epoch\"):\n    # TRAIN loop\n    model.train()\n    epoch_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Clearing previouly calculated gradients before backward pass\n        model.zero_grad()\n\n        # forward pass\n        output = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask, labels = b_labels)\n\n        # backward pass\n        output.backward()\n\n        # track train loss\n        epoch_loss += output.item()\n        \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(parameters = model.parameters(), max_norm = max_grad_norm)\n        \n        # update parameters\n        optimizer.step()\n        \n    # print train loss per epoch\n    print(\"Training loss in Epoch #{} = {}\".format(_ + 1, epoch_loss))","metadata":{"_uuid":"e1b563e636d25bf1387da08cd2772bff700f8669","id":"Bz79TgD69TR9","outputId":"b7f553a2-42dc-4e57-b87a-295065997b01","execution":{"iopub.status.busy":"2021-11-06T16:47:05.316347Z","iopub.execute_input":"2021-11-06T16:47:05.316649Z","iopub.status.idle":"2021-11-06T17:00:12.924783Z","shell.execute_reply.started":"2021-11-06T16:47:05.316608Z","shell.execute_reply":"2021-11-06T17:00:12.923774Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Epoch:  20%|██        | 1/5 [02:38<10:33, 158.29s/it]","output_type":"stream"},{"name":"stdout","text":"Training loss in Epoch #1 = 871.8930041193962\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  40%|████      | 2/5 [05:15<07:53, 157.73s/it]","output_type":"stream"},{"name":"stdout","text":"Training loss in Epoch #2 = 415.67585867643356\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  60%|██████    | 3/5 [07:52<05:15, 157.56s/it]","output_type":"stream"},{"name":"stdout","text":"Training loss in Epoch #3 = 344.6825974583626\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  80%|████████  | 4/5 [10:30<02:37, 157.44s/it]","output_type":"stream"},{"name":"stdout","text":"Training loss in Epoch #4 = 320.44174218177795\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|██████████| 5/5 [13:07<00:00, 157.52s/it]","output_type":"stream"},{"name":"stdout","text":"Training loss in Epoch #5 = 309.0880976319313\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#save your model after the final epoch\ntorch.save(model.state_dict(), \"saved_model.h5\")","metadata":{"id":"LlNzwqfiGmFS","execution":{"iopub.status.busy":"2021-11-06T17:00:32.436751Z","iopub.execute_input":"2021-11-06T17:00:32.437028Z","iopub.status.idle":"2021-11-06T17:00:33.299322Z","shell.execute_reply.started":"2021-11-06T17:00:32.436998Z","shell.execute_reply":"2021-11-06T17:00:33.298383Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Retain the model parameters into the new bert-base-uncased model\nnew_bert_model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\nnew_bert_model.cuda()\nnew_bert_model.load_state_dict(torch.load(\"saved_model.h5\"))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T17:00:52.280903Z","iopub.execute_input":"2021-11-06T17:00:52.281210Z","iopub.status.idle":"2021-11-06T17:01:01.752231Z","shell.execute_reply.started":"2021-11-06T17:00:52.281180Z","shell.execute_reply":"2021-11-06T17:01:01.751335Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# evaluate the saved model on the validation set and predict the value also\nnew_bert_model.eval()\npredicted, actual = [], []\n\nfor batch in valid_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    with torch.no_grad():\n        output = new_bert_model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n\n        for prediction in output.detach().cpu().numpy():\n            predicted.append(prediction)\n\n        for label in b_labels.to('cpu').numpy():\n            actual.append(label)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T17:23:20.766948Z","iopub.execute_input":"2021-11-06T17:23:20.767277Z","iopub.status.idle":"2021-11-06T17:23:26.361512Z","shell.execute_reply.started":"2021-11-06T17:23:20.767215Z","shell.execute_reply":"2021-11-06T17:23:26.360368Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Printing Validation Flat Accuracy and Validation f1 score\n\npredicted_for_f1 = np.argmax(np.array(predicted), axis = 2).flatten().tolist()\nactual_for_f1 = np.array(actual).flatten().tolist()\n\nprint(\"Validation Flat Accuracy = {:.6f} %\".format(100 * flat_accuracy(np.array(predicted), np.array(actual))))\n\nimport sklearn\nprint(\"Validation F1-score =\", sklearn.metrics.f1_score(predicted_for_f1, actual_for_f1, average = 'weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:04:12.496992Z","iopub.execute_input":"2021-11-06T18:04:12.497401Z","iopub.status.idle":"2021-11-06T18:04:12.919998Z","shell.execute_reply.started":"2021-11-06T18:04:12.497344Z","shell.execute_reply":"2021-11-06T18:04:12.919032Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Validation Flat Accuracy = 89.386015 %\nValidation F1-score = 0.9408596620880114\n","output_type":"stream"}]},{"cell_type":"code","source":"#Save the  model and result\n\n# The model has already been saved before as \"saved_model.h5\"\n#Saving results\n\nfile = open('actual_predicted.csv', 'w')\nfile.write('Actual,Predicted')\n\nfor i in range(len(actual_for_f1)):\n    file.write(str(actual_for_f1[i]) + ',' + str(predicted_for_f1[i]) + '\\n')\n\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2021-11-06T18:13:31.348385Z","iopub.execute_input":"2021-11-06T18:13:31.348666Z","iopub.status.idle":"2021-11-06T18:13:31.513618Z","shell.execute_reply.started":"2021-11-06T18:13:31.348636Z","shell.execute_reply":"2021-11-06T18:13:31.512510Z"},"trusted":true},"execution_count":60,"outputs":[]}]}