{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18CS30032_Assn1_NLP_A21.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z_wN2v1RT1F"
      },
      "source": [
        "# **Assignment-1 for CS60075: Natural Language Processing**\n",
        "\n",
        "#### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
        "\n",
        "#### Date of Announcement: 4th Sept, 2021\n",
        "#### Deadline for Submission: 11.59pm on Sunday, 12th Sept, 2021 \n",
        "\n",
        "#### (**NOTE**: Submit a .zip file, containing this .ipynb file, named as `<Your_Roll_Number>_Assn1_NLP_A21.ipynb` and the raw text corpus named `<Your_Roll_Number>_Assn1_rawCorpus.txt`. For example, if your roll number is 20XX12Y45, name the .ipynb file as `20XX12Y45_Assn1_NLP_A21.ipynb`. Name the .zip as `<Your_Roll_Number>_Assn1_NLP_A21.zip`. Write your code in the respective designated portion of the .ipynb. Also before submitting, make sure that all the outputs of your code are present in the .ipynb file itself.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a35tmEySCx7"
      },
      "source": [
        "### **Submission Details:**\n",
        "Name: Rahul Aditya\n",
        "\n",
        "Roll No.: 18CS30032\n",
        "\n",
        "Department: CSE\n",
        "\n",
        "Email-ID: rahul.aditya180@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9weHMmyd8fnq"
      },
      "source": [
        "## **Reading a Raw Text Corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmSy_LOK2aGQ"
      },
      "source": [
        "Retrieve & save raw corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rku6rV2ORpZA"
      },
      "source": [
        "# To construct your corpus, retrieve (through Python code) Chapter I to Chapter X,\n",
        "# both inclusive, from the link below:\n",
        "# \"https://www.gutenberg.org/files/730/730-0.txt\"\n",
        "# Save this corpus in a text file, named as 'rawCorpus.txt'\n",
        "# Print the total number of characters in the text file \n",
        "\n",
        "# *** Write code ***\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/730/730-0.txt\"\n",
        "html_file = urlopen(url).read()\n",
        "soup = BeautifulSoup(html_file, features = \"html.parser\")\n",
        "\n",
        "for data in soup([\"script\", \"style\"]):\n",
        "    data.extract()\n",
        "\n",
        "text = soup.get_text()\n",
        "\n",
        "chapter1 = text.index(\"CHAPTER I.\")\n",
        "chapter11 = text.index(\"CHAPTER XI.\")\n",
        "\n",
        "filtered_text = text[chapter1 : chapter11]\n",
        "actual_text = filtered_text.strip()\n",
        "file_object = open(\"18CS30032_Assn1_rawCorpus.txt\", \"w\")\n",
        "file_object.write(actual_text)\n",
        "file_object.close()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZIOy0Y2hzQ"
      },
      "source": [
        "Read the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsdBJa_l2l7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b0e5f8-12f2-4f41-f0b8-2c85157d5b97"
      },
      "source": [
        "# Read the corpus from rawCorpus.txt, in a variable `rawReadCorpus`\n",
        "# *** Write code ***\n",
        "with open(\"18CS30032_Assn1_rawCorpus.txt\", \"r\") as text_file:\n",
        "    rawReadCorpus = text_file.read()\n",
        "print (\"Total # of characters in read dataset: {}\".format(len(rawReadCorpus)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total # of characters in read dataset: 148711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhkmGsSoV0zG"
      },
      "source": [
        "## **Installing NLTK**\n",
        "\n",
        "The Natural Language Toolkit ([NLTK](https://www.nltk.org/)) is a Python module that is intended to support research and teaching in NLP or closely related areas. \n",
        "\n",
        "Detailed installation instructions to install NLTK can be found at this [link](https://www.nltk.org/install.html).\n",
        "\n",
        "To ensure uniformity, we suggest to use **python3**. You can download Anaconda3 and create a separate environment to do this assignment, eg.\n",
        "```bash\n",
        "conda create -n myenv python=3.6\n",
        "conda activate myenv\n",
        "```\n",
        "\n",
        "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. Subsequently, you can install NLTK through the following commands:\n",
        "```bash\n",
        "sudo pip3 install nltk \n",
        "python3 \n",
        "nltk.download()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKtZeHq4N98"
      },
      "source": [
        "## **Preprocessing the corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LSUX__82Ff"
      },
      "source": [
        "**Tokenize into words and sentences, using NLTK library:** Using the NLTK modules imported above, retrieve a case-insensitive preprocessed model. Make sure to take care of words like \"\\_will\\_\" (that should ideally appear as \"will\"), \"wouldn't\" (that should ideally appear as a single word, and not multiple tokens) and other occurences of special cases that you find in the raw corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g7eO4Dm4jIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63af59e1-ceb8-45c7-c0f3-3043020d0ad4"
      },
      "source": [
        "# Importing modules\n",
        "import nltk\n",
        "nltk.download('punkt') # For tokenizers\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWIzYXyz9Zt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398fe316-1b80-4be3-d4cd-257a532aca73"
      },
      "source": [
        "# *** Write code for preprocessing the corpus ***\n",
        "import string\n",
        "allPunctuators = string.punctuation\n",
        "allPunctuators += \"—\"\n",
        "allPunctuators += '“'\n",
        "allPunctuators += '”'\n",
        "allPunctuators += \"’\"\n",
        "allPunctuators += \"‘\"\n",
        "\n",
        "def preprocess(sentence):\n",
        "  pp_sentence = \"\"\n",
        "  for ch in sentence:\n",
        "    if ch not in allPunctuators:\n",
        "      pp_sentence += ch\n",
        "  return pp_sentence.lower()\n",
        "\n",
        "tokens_for_words = word_tokenize(preprocess(rawReadCorpus))\n",
        "tokens_for_sentences_temp = sent_tokenize(rawReadCorpus)\n",
        "tokens_for_sentences = []\n",
        "\n",
        "for sentence in tokens_for_sentences_temp:\n",
        "  sentence = preprocess(sentence)\n",
        "  tokens_for_sentences.append(sentence)\n",
        "\n",
        "# Print first 5 sentences of your preprocessed corpus *** Write code ***\n",
        "print(\"First 5 sentences\\n\")\n",
        "for i in range(5):\n",
        "  print(\"Sentence #\", i + 1)\n",
        "  print(tokens_for_sentences[i])\n",
        "  print(\"\\n\")\n",
        "# Print first 5 words/tokens of your preprocessed corpus *** Write code ***\n",
        "print(\"\\nFirst 5 words\\n\")\n",
        "for i in range(5):\n",
        "  print(tokens_for_words[i])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 sentences\n",
            "\n",
            "Sentence # 1\n",
            "chapter i\n",
            "\n",
            "\n",
            "Sentence # 2\n",
            "treats of the place where oliver twist was born and of the\n",
            "circumstances attending his birth\n",
            "\n",
            "\n",
            "among other public buildings in a certain town which for many reasons\n",
            "it will be prudent to refrain from mentioning and to which i will\n",
            "assign no fictitious name there is one anciently common to most towns\n",
            "great or small to wit a workhouse and in this workhouse was born on\n",
            "a day and date which i need not trouble myself to repeat inasmuch as\n",
            "it can be of no possible consequence to the reader in this stage of\n",
            "the business at all events the item of mortality whose name is\n",
            "prefixed to the head of this chapter\n",
            "\n",
            "\n",
            "Sentence # 3\n",
            "for a long time after it was ushered into this world of sorrow and\n",
            "trouble by the parish surgeon it remained a matter of considerable\n",
            "doubt whether the child would survive to bear any name at all in which\n",
            "case it is somewhat more than probable that these memoirs would never\n",
            "have appeared or if they had that being comprised within a couple of\n",
            "pages they would have possessed the inestimable merit of being the\n",
            "most concise and faithful specimen of biography extant in the\n",
            "literature of any age or country\n",
            "\n",
            "\n",
            "Sentence # 4\n",
            "although i am not disposed to maintain that the being born in a\n",
            "workhouse is in itself the most fortunate and enviable circumstance\n",
            "that can possibly befall a human being i do mean to say that in this\n",
            "particular instance it was the best thing for oliver twist that could\n",
            "by possibility have occurred\n",
            "\n",
            "\n",
            "Sentence # 5\n",
            "the fact is that there was considerable\n",
            "difficulty in inducing oliver to take upon himself the office of\n",
            "respirationa troublesome practice but one which custom has rendered\n",
            "necessary to our easy existence and for some time he lay gasping on a\n",
            "little flock mattress rather unequally poised between this world and\n",
            "the next the balance being decidedly in favour of the latter\n",
            "\n",
            "\n",
            "\n",
            "First 5 words\n",
            "\n",
            "chapter\n",
            "i\n",
            "treats\n",
            "of\n",
            "the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ75_a1QL70J"
      },
      "source": [
        "**Perform the following tasks for the given corpus:**\n",
        "1. Print the average number of tokens per sentence.\n",
        "2. Print the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\n",
        "3. Print the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyG0g3oSADmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8933fe1c-1833-4d25-ce5a-0a6e6bda63a9"
      },
      "source": [
        "# Importing modules\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "list_of_stop_words = stopwords.words('english')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydHIxC7lG7Py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef21a89-2b70-4ab8-fee7-e167755a2f20"
      },
      "source": [
        "# *** Write code for the 2 tasks above ***\n",
        "avg_tokens = len(tokens_for_words) / len(tokens_for_sentences)\n",
        "print(\"Average tokens = \", str(avg_tokens))\n",
        "\n",
        "sentences_Oliver = []\n",
        "for sentence in tokens_for_sentences_temp:\n",
        "  if \"oliver\" in sentence.lower():\n",
        "    sentences_Oliver.append(sentence)\n",
        "\n",
        "len_max = len(sentences_Oliver[0])\n",
        "len_min = len(sentences_Oliver[0])\n",
        "\n",
        "for sentence in sentences_Oliver:\n",
        "  cur = len(sentence)\n",
        "  len_max = max(len_max, cur)\n",
        "  len_min = min(len_min, cur)\n",
        "\n",
        "print(\"The length of the longest sentence that contains Oliver = \", len_max)\n",
        "print(\"The length of the shortest sentence that contains Oliver = \", len_min)\n",
        "\n",
        "vocab_no_stopwords = set({})\n",
        "for token in tokens_for_words:\n",
        "  if token not in list_of_stop_words:\n",
        "    vocab_no_stopwords.add(token)\n",
        "\n",
        "print(\"The number of unique = \", len(vocab_no_stopwords))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average tokens =  23.79634703196347\n",
            "The length of the longest sentence that contains Oliver =  643\n",
            "The length of the shortest sentence that contains Oliver =  12\n",
            "The number of unique =  4211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RiDR7TJjKX"
      },
      "source": [
        "## **Language Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJeTSt8HM95L"
      },
      "source": [
        "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
        "\n",
        "1. **Create the following language models** on the given corpus: <br>\n",
        "    i.   Unigram <br>\n",
        "    ii.  Bigram <br>\n",
        "    iii. Trigram <br>\n",
        "\n",
        "2. **List the top 10 bigrams, trigrams**\n",
        "(Additionally remove those items which contain only articles, prepositions, determiners eg. \"of the\", \"in a\", etc. List top-10 bigrams/trigrams in both the original and processed models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlPXGvVaR-ka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa976d6-c74a-42c5-cfcc-7dcc90b361f7"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "unigrams=[]\n",
        "bigrams=[]\n",
        "trigrams=[]\n",
        "\n",
        "for content in tokens_for_sentences:\n",
        "  unigrams.extend(word_tokenize(content))\n",
        "  bigrams.extend(ngrams(word_tokenize(content),2))\n",
        "\n",
        "  ##similar for trigrams \n",
        "  # *** Write code ***\n",
        "  number_of_tokens = len(word_tokenize(content))\n",
        "  if number_of_tokens > 1:\n",
        "    trigrams.extend(ngrams(word_tokenize(content), 3))\n",
        "\n",
        "print (\"Sample of n-grams:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigrams[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigrams[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigrams[:5]) + \" ...\\n\")\n",
        "\n",
        "# list of unigram, bigram & trigram after removing those that \n",
        "# totally contain only articles, prepositions, determiners\n",
        "# Eg. For bigrams, don't remove items like (\"a\", \"boy\") --> where not all are \n",
        "#     articles, prepositions, determiners\n",
        "#     But remove items like (\"in\", \"the\") --> where all are articles, prepositions, determiners\n",
        "# Similarly, for unigrams and trigrams\n",
        "unigrams_Processed = []\n",
        "bigrams_Processed = []\n",
        "trigrams_Processed = []\n",
        "\n",
        "for token in unigrams:\n",
        "  if token not in list_of_stop_words:\n",
        "    unigrams_Processed.append(token)\n",
        "\n",
        "for bigram_tuple in bigrams:\n",
        "  ok = False\n",
        "  if bigram_tuple[0] not in list_of_stop_words:\n",
        "    ok = True\n",
        "  if bigram_tuple[1] not in list_of_stop_words:\n",
        "    ok = True\n",
        "  if ok:\n",
        "    bigrams_Processed.append(bigram_tuple)\n",
        "\n",
        "for trigram_tuple in trigrams:\n",
        "  ok = False\n",
        "  if trigram_tuple[0] not in list_of_stop_words:\n",
        "    ok = True\n",
        "  if trigram_tuple[1] not in list_of_stop_words:\n",
        "    ok = True\n",
        "  if trigram_tuple[2] not in list_of_stop_words:\n",
        "    ok = True\n",
        "  if ok:\n",
        "    trigrams_Processed.append(trigram_tuple)\n",
        "\n",
        "\n",
        "print (\"Sample of n-grams after processing:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigrams_Processed[:5]) + \" ...\\n\")\n",
        "\n",
        "from collections import Counter\n",
        "def get_ngrams_freqDist(n, ngramList):\n",
        "    #This function computes the frequency corresponding to each ngram in ngramList \n",
        "    #Here, n=1 for unigram, n=2 for bigram, etc.\n",
        "    #ngramList = list of unigrams when n=1, ngramList = list of bigrams when n=2\n",
        "    #Returns: ngram_freq_dict (a Python dictionary where key = a ngram, value = its frequency)\n",
        "    \n",
        "    # *** Write code ***\n",
        "    ngram_freq_dict = Counter()\n",
        "    for gram in ngramList:\n",
        "      if gram in ngram_freq_dict:\n",
        "        ngram_freq_dict[gram] += 1\n",
        "      else:\n",
        "        ngram_freq_dict[gram] = 1\n",
        "    return ngram_freq_dict\n",
        "\n",
        "unigrams_freqDist = get_ngrams_freqDist(1, unigrams)\n",
        "unigrams_Processed_freqDist = get_ngrams_freqDist(1, unigrams_Processed)\n",
        "bigrams_freqDist = get_ngrams_freqDist(2, bigrams)\n",
        "bigrams_Processed_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
        "trigrams_freqDist = get_ngrams_freqDist(3, trigrams)\n",
        "trigrams_Processed_freqDist = get_ngrams_freqDist(3, trigrams_Processed)                                                 \n",
        "\n",
        "# Print top 10 unigrams, having highest frequency as in unigrams_freqDist\n",
        "# *** Write code ***\n",
        "print(\"\\nTop 10 unigrams, having highest frequency as in unigrams_freqDist\\n\", unigrams_freqDist.most_common()[:10])\n",
        "\n",
        "# Print top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\n",
        "# *** Write code ***\n",
        "print(\"\\nTop 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\\n\", unigrams_Processed_freqDist.most_common()[:10])\n",
        "\n",
        "# Print top 10 bigrams, having highest frequency as in bigrams_freqDist\n",
        "# *** Write code ***\n",
        "print(\"\\nTop 10 bigrams, having highest frequency as in bigrams_freqDist\\n\", bigrams_freqDist.most_common()[:10])\n",
        "\n",
        "# Print top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\n",
        "# *** Write code ***\n",
        "print(\"\\nTop 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\\n\", bigrams_Processed_freqDist.most_common()[:10])\n",
        "\n",
        "# Print top 10 trigrams, having highest frequency as in trigrams_freqDist\n",
        "# *** Write code ***\n",
        "print(\"\\nTop 10 trigrams, having highest frequency as in trigrams_freqDist\\n\", trigrams_freqDist.most_common()[:10])\n",
        "\n",
        "# Print top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\n",
        "# *** Write code ***\n",
        "print(\"\\nTop 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\\n\", trigrams_Processed_freqDist.most_common()[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of n-grams:\n",
            "-------------------------\n",
            "--> UNIGRAMS: \n",
            "['chapter', 'i', 'treats', 'of', 'the'] ...\n",
            "\n",
            "--> BIGRAMS: \n",
            "[('chapter', 'i'), ('treats', 'of'), ('of', 'the'), ('the', 'place'), ('place', 'where')] ...\n",
            "\n",
            "--> TRIGRAMS: \n",
            "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
            "\n",
            "Sample of n-grams after processing:\n",
            "-------------------------\n",
            "--> UNIGRAMS: \n",
            "['chapter', 'treats', 'place', 'oliver', 'twist'] ...\n",
            "\n",
            "--> BIGRAMS: \n",
            "[('chapter', 'i'), ('treats', 'of'), ('the', 'place'), ('place', 'where'), ('where', 'oliver')] ...\n",
            "\n",
            "--> TRIGRAMS: \n",
            "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
            "\n",
            "\n",
            "Top 10 unigrams, having highest frequency as in unigrams_freqDist\n",
            " [('the', 1701), ('and', 856), ('a', 713), ('of', 673), ('to', 617), ('his', 455), ('he', 449), ('in', 441), ('was', 368), ('oliver', 277)]\n",
            "\n",
            "Top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\n",
            " [('oliver', 277), ('said', 212), ('mr', 191), ('bumble', 123), ('gentleman', 102), ('old', 89), ('would', 77), ('sowerberry', 77), ('replied', 74), ('boy', 74)]\n",
            "\n",
            "Top 10 bigrams, having highest frequency as in bigrams_freqDist\n",
            " [(('of', 'the'), 162), (('in', 'the'), 127), (('mr', 'bumble'), 107), (('to', 'the'), 91), (('said', 'the'), 90), (('he', 'had'), 67), (('he', 'was'), 62), (('on', 'the'), 60), (('in', 'a'), 55), (('with', 'a'), 54)]\n",
            "\n",
            "Top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\n",
            " [(('mr', 'bumble'), 107), (('said', 'the'), 90), (('the', 'old'), 53), (('old', 'gentleman'), 39), (('the', 'undertaker'), 36), (('said', 'mr'), 35), (('the', 'boy'), 35), (('the', 'gentleman'), 33), (('the', 'jew'), 33), (('mr', 'sowerberry'), 32)]\n",
            "\n",
            "Top 10 trigrams, having highest frequency as in trigrams_freqDist\n",
            " [(('the', 'old', 'gentleman'), 29), (('gentleman', 'in', 'the'), 22), (('the', 'gentleman', 'in'), 20), (('the', 'white', 'waistcoat'), 20), (('said', 'mr', 'bumble'), 19), (('in', 'the', 'white'), 18), (('said', 'the', 'gentleman'), 14), (('said', 'the', 'undertaker'), 14), (('said', 'the', 'jew'), 14), (('sir', 'replied', 'oliver'), 12)]\n",
            "\n",
            "Top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\n",
            " [(('the', 'old', 'gentleman'), 29), (('gentleman', 'in', 'the'), 22), (('the', 'gentleman', 'in'), 20), (('the', 'white', 'waistcoat'), 20), (('said', 'mr', 'bumble'), 19), (('in', 'the', 'white'), 18), (('said', 'the', 'gentleman'), 14), (('said', 'the', 'undertaker'), 14), (('said', 'the', 'jew'), 14), (('sir', 'replied', 'oliver'), 12)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqu8nVV7NREo"
      },
      "source": [
        "## **Next three words' Prediction using Smoothed Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2vnIM26b2WA"
      },
      "source": [
        "For a bigram model, add-one smoothing is defined by $P_{Add-1}(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V}$.\n",
        "That is, pretend we saw each word one more time than we did.\n",
        "\n",
        "You have two tasks here.\n",
        "\n",
        "First, compute the smoothed bigram and trigram models from the bigrams_freqDist and trigrams_freqDist you calculated above (use the unprocessed models). Second, using these smoothed models, predict the next 3 possible word sequences for testSent1, testSent2 and testSent3, using your smoothed models.\n",
        "\n",
        "As for example, for the string 'Raj has a' the answers can be as below: \n",
        "\n",
        "(1) Raj has a **beautiful red car**\n",
        "\n",
        "(2) Raj has a **charismatic magnetic personality**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0oAhfy3LRbO"
      },
      "source": [
        "testSent1 = \"There was a sudden jerk, a terrific convulsion of the limbs; and there he\"\n",
        "testSent2 = \"They made room for the stranger, but he sat down\"\n",
        "testSent3 = \"The hungry and destitute situation of the infant orphan was duly reported by\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLY1ymH-ZuJu"
      },
      "source": [
        "# *** Write code ***\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "Bigram_prediction = defaultdict(dict)\n",
        "Trigram_prediction = defaultdict(dict)\n",
        "\n",
        "vocab_size = len(unigrams_freqDist)\n",
        "\n",
        "for bigram_tuple, count in bigrams_freqDist.items():\n",
        "  first_member_count = unigrams_freqDist[bigram_tuple[0]]\n",
        "  Bigram_prediction[bigram_tuple[0]][bigram_tuple[1]] = (count + 1) / (first_member_count + vocab_size)\n",
        "\n",
        "for trigram_tuple, count in trigrams_freqDist.items():\n",
        "  first_second_member_count = bigrams_freqDist[(trigram_tuple[0], trigram_tuple[1])]\n",
        "  denominator = first_second_member_count + vocab_size\n",
        "  Trigram_prediction[(trigram_tuple[0], trigram_tuple[1])][trigram_tuple[2]] = (count + 1) / denominator\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEtde33MDCA3"
      },
      "source": [
        "def predict_next_bigram(words):\n",
        "  ans = []\n",
        "  last_word = words[-1]\n",
        "\n",
        "  for i in range(3):\n",
        "    next_word = \"\"\n",
        "    prob_of_next_word = -1.0\n",
        "    for W, prob_of_W in Bigram_prediction[last_word].items():\n",
        "      if prob_of_W > prob_of_next_word:\n",
        "        next_word = W\n",
        "        prob_of_next_word = prob_of_W\n",
        "    ans.append(next_word)\n",
        "    last_word = next_word\n",
        "  \n",
        "  return ans\n",
        "\n",
        "def predict_next_trigram(words):\n",
        "  ans = []\n",
        "  last_word = words[-1]\n",
        "  second_last_word = words[-2]\n",
        "\n",
        "  for i in range(3):\n",
        "    next_word = \"\"\n",
        "    prob_of_next_word = -1.0\n",
        "    for W, prob_of_W in Trigram_prediction[(second_last_word, last_word)].items():\n",
        "      if prob_of_W > prob_of_next_word:\n",
        "        next_word = W\n",
        "        prob_of_next_word = prob_of_W\n",
        "    ans.append(next_word)\n",
        "    second_last_word = last_word\n",
        "    last_word = next_word\n",
        "  \n",
        "  return ans"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WsD36_sGrpM",
        "outputId": "1213817a-d921-47ce-d1b3-eb334c74d60e"
      },
      "source": [
        "def find_next_3_words_bigram(text):\n",
        "  tokens_of_text = word_tokenize(preprocess(text))\n",
        "  next_words = predict_next_bigram(tokens_of_text)\n",
        "\n",
        "  final_text = \"\"\n",
        "  for next_word in next_words:\n",
        "    final_text = final_text + \" \" + next_word\n",
        "  final_text = text + final_text\n",
        "  return final_text\n",
        "\n",
        "def find_next_3_words_trigram(text):\n",
        "  tokens_of_text = word_tokenize(preprocess(text))\n",
        "  next_words = predict_next_trigram(tokens_of_text)\n",
        "\n",
        "  final_text = \"\"\n",
        "  for next_word in next_words:\n",
        "    final_text = final_text + \" \" + next_word\n",
        "  final_text = text + final_text\n",
        "  return final_text\n",
        "\n",
        "ans_bigram = \"\"\n",
        "ans_trigram = \"\"\n",
        "\n",
        "print(\"Prediction for testSent1\\n\")\n",
        "ans_bigram1 = find_next_3_words_bigram(testSent1)\n",
        "ans_trigram1 = find_next_3_words_trigram(testSent1)\n",
        "print(\"Original = \" + testSent1)\n",
        "print(\"Bigram model: \", ans_bigram1)\n",
        "print(\"Trigram model: \", ans_trigram1)\n",
        "\n",
        "print(\"\\nPrediction for testSent2\\n\")\n",
        "ans_bigram2 = find_next_3_words_bigram(testSent2)\n",
        "ans_trigram2 = find_next_3_words_trigram(testSent2)\n",
        "print(\"Original = \" + testSent2)\n",
        "print(\"Bigram model: \", ans_bigram2)\n",
        "print(\"Trigram model: \", ans_trigram2)\n",
        "\n",
        "print(\"\\nPrediction for testSent3\\n\")\n",
        "ans_bigram3 = find_next_3_words_bigram(testSent3)\n",
        "ans_trigram3 = find_next_3_words_trigram(testSent3)\n",
        "print(\"Original = \" + testSent3)\n",
        "print(\"Bigram model: \", ans_bigram3)\n",
        "print(\"Trigram model: \", ans_trigram3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for testSent1\n",
            "\n",
            "Original = There was a sudden jerk, a terrific convulsion of the limbs; and there he\n",
            "Bigram model:  There was a sudden jerk, a terrific convulsion of the limbs; and there he had been expected\n",
            "Trigram model:  There was a sudden jerk, a terrific convulsion of the limbs; and there he sat down to\n",
            "\n",
            "Prediction for testSent2\n",
            "\n",
            "Original = They made room for the stranger, but he sat down\n",
            "Bigram model:  They made room for the stranger, but he sat down the old gentleman\n",
            "Trigram model:  They made room for the stranger, but he sat down to a branchworkhouse\n",
            "\n",
            "Prediction for testSent3\n",
            "\n",
            "Original = The hungry and destitute situation of the infant orphan was duly reported by\n",
            "Bigram model:  The hungry and destitute situation of the infant orphan was duly reported by the old gentleman\n",
            "Trigram model:  The hungry and destitute situation of the infant orphan was duly reported by the side of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfeaacTdO6h"
      },
      "source": [
        "Check the presence of these sentences in the original corpus at https://www.gutenberg.org/files/730/730-0.txt . How did your smoothed models perform in comparison to the original sentences? Compare them below.\n",
        "\n",
        "Did you notice something special about testSent3, in comparison to testSent1 and testSent2? If yes, what is it? Can you explain it?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFMkW9hKecxK"
      },
      "source": [
        "Leaving aside the results obtained for testSent3 where both the Bigram and Trigram model were able to partially predict the correct answer, the results obtained didn't match at all for testSent1 and testSent2. However, the results obtained for all the 3 sentences, for each of Bigram and Trigram model, were satisfactory in the sense that we got meaningful words.\n",
        "\n",
        "In case of testSent3, both Bigram and Trigram models were able to correctly predict the first word which also matches with the first word following testSent3 in the corpus. The correctly predicted word is \"the\". It happened because there is a very high probability of the word \"the\" to come after \"by\" and since both Bigram and Trigram models capture the previous word, the correct word \"the\" has been predicted. This is not the case with testSent1 and testSent2 where the next word cannot be easily predicted given the last word in case of Bigram model and the last two words in case of Trigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVBMcaAJXR9S"
      },
      "source": [
        "Which of the three models you generated above (unigram, bigram, trigram) is better in terms of **perplexity**, for the three test sentences (unseen data)? Write a piece of code to justify your answer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAPa1OVZX8uN"
      },
      "source": [
        "The code has been written in the next cell. The best model for testSent1 and testSent2 is Unigram model because of significantly low preplexity as compared to Bigram and Trigram models. In case of testSent3, both Unigram model and Trigram model performed good but Trigram model is a bit better in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4-PVvF2Te2N",
        "outputId": "19341b6b-4941-40b7-ccd8-384ed5eb423f"
      },
      "source": [
        "# find the unigram model\n",
        "Unigram_prediction = defaultdict(dict)\n",
        "vocab_size = len(unigrams_freqDist)\n",
        "\n",
        "for unigram, count in unigrams_freqDist.items():\n",
        "  Unigram_prediction[unigram] = (count + 1) / (len(tokens_for_words) + vocab_size)\n",
        "\n",
        "def find_perplexity(text):\n",
        "  text_tokens = word_tokenize(preprocess(text))\n",
        "  unigrams, bigrams, trigrams = [], [], []\n",
        "\n",
        "  unigrams.extend(text_tokens)\n",
        "  bigrams.extend(ngrams(text_tokens, 2))\n",
        "  trigrams.extend(ngrams(text_tokens, 3))\n",
        "\n",
        "  logPerplexityUnigram = 0\n",
        "  logPerplexityBigram = 0\n",
        "  logPerplexityTrigram = 0\n",
        "\n",
        "  vocab_size = len(unigrams_freqDist)\n",
        "\n",
        "  for unigram in unigrams:\n",
        "    if unigram not in Unigram_prediction:\n",
        "      logPerplexityUnigram -= math.log10(1 / (len(text_tokens) + vocab_size))\n",
        "    else:\n",
        "      logPerplexityUnigram -= math.log10(Unigram_prediction[unigram])\n",
        "  \n",
        "  logPerplexityUnigram /= len(text_tokens)\n",
        "  \n",
        "  for bigram in bigrams:\n",
        "    if bigram[0] in Bigram_prediction and bigram[1] in Bigram_prediction[bigram[0]]:\n",
        "      logPerplexityBigram -= math.log10(Bigram_prediction[bigram[0]][bigram[1]])\n",
        "    else:\n",
        "      logPerplexityBigram -= math.log10(1 / (unigrams_freqDist[bigram[0]] + vocab_size))\n",
        "  \n",
        "  logPerplexityBigram /= len(text_tokens)\n",
        "\n",
        "  for trigram in trigrams:\n",
        "    if (trigram[0], trigram[1]) in Trigram_prediction and trigram[2] in Trigram_prediction[(trigram[0], trigram[1])]:\n",
        "      logPerplexityTrigram -= math.log10(Trigram_prediction[(trigram[0], trigram[1])][trigram[2]])\n",
        "    else:\n",
        "      logPerplexityTrigram -= math.log10(1 / (bigrams_freqDist[(trigram[0], trigram[1])] + vocab_size))\n",
        "  \n",
        "  logPerplexityTrigram /= len(text_tokens)\n",
        "\n",
        "  return pow(10, logPerplexityUnigram), pow(10, logPerplexityBigram), pow(10, logPerplexityTrigram)\n",
        "\n",
        "print(\"Perplexity calculations are as follows\\n\")\n",
        "Unigram1, Bigram1, Trigram1 = find_perplexity(testSent1)\n",
        "print(\"For testSent1:\")\n",
        "print(\"[Unigram Model] Perplexity = \", Unigram1)\n",
        "print(\"[Bigram Model] Perplexity = \", Bigram1)\n",
        "print(\"[Trigram Model] Perplexity = \", Trigram1)\n",
        "\n",
        "Unigram2, Bigram2, Trigram2 = find_perplexity(testSent2)\n",
        "print(\"\\nFor testSent2:\")\n",
        "print(\"[Unigram Model] Perplexity = \", Unigram2)\n",
        "print(\"[Bigram Model] Perplexity = \", Bigram2)\n",
        "print(\"[Trigram Model] Perplexity = \", Trigram2)\n",
        "\n",
        "Unigram3, Bigram3, Trigram3 = find_perplexity(testSent3)\n",
        "print(\"\\nFor testSent3:\")\n",
        "print(\"[Unigram Model] Perplexity = \", Unigram3)\n",
        "print(\"[Bigram Model] Perplexity = \", Bigram3)\n",
        "print(\"[Trigram Model] Perplexity = \", Trigram3)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity calculations are as follows\n",
            "\n",
            "For testSent1:\n",
            "[Unigram Model] Perplexity =  319.3819366855145\n",
            "[Bigram Model] Perplexity =  761.3339271304039\n",
            "[Trigram Model] Perplexity =  1057.076323732639\n",
            "\n",
            "For testSent2:\n",
            "[Unigram Model] Perplexity =  434.04036314588865\n",
            "[Bigram Model] Perplexity =  645.8669860251555\n",
            "[Trigram Model] Perplexity =  727.7756803458261\n",
            "\n",
            "For testSent3:\n",
            "[Unigram Model] Perplexity =  698.4333669447461\n",
            "[Bigram Model] Perplexity =  812.8777115117176\n",
            "[Trigram Model] Perplexity =  666.14492819878\n"
          ]
        }
      ]
    }
  ]
}